{
    "nbformat_minor": 0, 
    "metadata": {
        "language_info": {
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "name": "ipython", 
                "version": 3
            }, 
            "file_extension": ".py", 
            "version": "3.5.2", 
            "mimetype": "text/x-python", 
            "name": "python", 
            "nbconvert_exporter": "python"
        }, 
        "kernelspec": {
            "name": "python3-spark20", 
            "language": "python", 
            "display_name": "Python 3.5 (Experimental) with Spark 2.0"
        }
    }, 
    "cells": [
        {
            "source": "# Spark Streaming com Twitter", 
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "source": "!pip install requests_oauthlib", 
            "execution_count": 142, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "text": "Requirement already satisfied (use --upgrade to upgrade): requests_oauthlib in /gpfs/global_fs01/sym_shared/YPProdSpark/user/sf2f-8f4bf40bb9b7e1-6bd8badf036a/.local/lib/python3.5/site-packages\r\nRequirement already satisfied (use --upgrade to upgrade): requests>=2.0.0 in /usr/local/src/conda3_runtime.v9/4.1.1/lib/python3.5/site-packages (from requests_oauthlib)\r\nRequirement already satisfied (use --upgrade to upgrade): oauthlib>=0.6.2 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/sf2f-8f4bf40bb9b7e1-6bd8badf036a/.local/lib/python3.5/site-packages (from requests_oauthlib)\r\n", 
                    "output_type": "stream"
                }
            ]
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "from pyspark.streaming import StreamingContext\nfrom pyspark import SparkContext\nfrom requests_oauthlib import OAuth1Session\nfrom operator import add\nimport requests_oauthlib\nfrom time import gmtime,strftime\nimport requests\nimport time\nimport string\nimport ast\nimport json", 
            "execution_count": 143, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "source": "#Pacote NLTK\nimport nltk\nfrom nltk.classify import NaiveBayesClassifier\nfrom nltk.sentiment import SentimentAnalyzer\nfrom nltk.corpus import subjectivity\nfrom nltk.corpus import stopwords\nfrom nltk.sentiment.util import *", 
            "execution_count": 144, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "source": "\n# @hidden_cell\ncredentials_1 = {\n  'auth_url':'https://identity.open.softlayer.com',\n  'project':'object_storage_f0d6ce32_5e0f_4bc0_8812_229b8d429dbe',\n  'project_id':'9a0cc60102244d368e96a83f25d4ca89',\n  'region':'dallas',\n  'user_id':'0caf8026c98a4342ac027a05416e6dee',\n  'domain_id':'3be46074545f4c09b1f10df3ace95998',\n  'domain_name':'1351407',\n  'username':'member_327b95c3eecf105b8bdb0125b81968cfcc557dbd',\n  'password':\"\"\"D[Cvr1bgf9DM^I{C\"\"\",\n  'container':'CursoSpark',\n  'tenantId':'undefined',\n  'filename':'dataset_analise_sentimento.csv'\n}\n", 
            "execution_count": 145, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "source": "def set_hadoop_config_with_credentials(creds):  \n    \"\"\"This function sets the Hadoop configuration so it is possible to\n    access data from Bluemix Object Storage using Spark\"\"\"\n\n    # you can choose any name\n    name = 'keystone'\n\n    prefix = 'fs.swift.service.' + name\n    hconf = sc._jsc.hadoopConfiguration()\n    hconf.set(prefix + '.auth.url', 'https://identity.open.softlayer.com'+'/v3/auth/tokens')\n    hconf.set(prefix + '.auth.endpoint.prefix', 'endpoints')\n    hconf.set(prefix + '.tenant', creds['project_id'])\n    hconf.set(prefix + '.username', creds['user_id'])\n    hconf.set(prefix + '.password', creds['password'])\n    hconf.setInt(prefix + '.http.port', 8080)\n    hconf.set(prefix + '.region', 'dallas')\n    hconf.setBoolean(prefix + '.public', False)", 
            "execution_count": 146, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "source": "\nfrom pyspark.sql import SparkSession\n\n# @hidden_cell\n# This function is used to setup the access of Spark to your Object Storage. The definition contains your credentials.\n# You might want to remove those credentials before you share your notebook.\ndef set_hadoop_config_with_credentials_f0d6ce325e0f4bc08812229b8d429dbe(name):\n    \"\"\"This function sets the Hadoop configuration so it is possible to\n    access data from Bluemix Object Storage using Spark\"\"\"\n\n    prefix = 'fs.swift.service.' + name\n    hconf = sc._jsc.hadoopConfiguration()\n    hconf.set(prefix + '.auth.url', 'https://identity.open.softlayer.com'+'/v3/auth/tokens')\n    hconf.set(prefix + '.auth.endpoint.prefix', 'endpoints')\n    hconf.set(prefix + '.tenant', '9a0cc60102244d368e96a83f25d4ca89')\n    hconf.set(prefix + '.username', '0caf8026c98a4342ac027a05416e6dee')\n    hconf.set(prefix + '.password', 'D[Cvr1bgf9DM^I{C')\n    hconf.setInt(prefix + '.http.port', 8080)\n    hconf.set(prefix + '.region', 'dallas')\n    hconf.setBoolean(prefix + '.public', False)\n\n# you can choose any name\nname = 'keystone'\nset_hadoop_config_with_credentials_f0d6ce325e0f4bc08812229b8d429dbe(name)\n\nspark = SparkSession.builder.getOrCreate()\n\ndf_data_1 = spark.read\\\n  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n  .option('header', 'true')\\\n  .load('swift://CursoSpark.' + name + '/dataset_analise_sentimento.csv')\n", 
            "execution_count": 147, 
            "cell_type": "code", 
            "outputs": [
                {
                    "evalue": "'NoneType' object has no attribute 'hadoopConfiguration'", 
                    "ename": "AttributeError", 
                    "traceback": [
                        "\u001b[1;31m\u001b[0m", 
                        "\u001b[1;31mAttributeError\u001b[0mTraceback (most recent call last)", 
                        "\u001b[1;32m<ipython-input-147-148c4c22c453>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m# you can choose any name\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'keystone'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0mset_hadoop_config_with_credentials_f0d6ce325e0f4bc08812229b8d429dbe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0mspark\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;32m<ipython-input-147-148c4c22c453>\u001b[0m in \u001b[0;36mset_hadoop_config_with_credentials_f0d6ce325e0f4bc08812229b8d429dbe\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mprefix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'fs.swift.service.'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mhconf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhadoopConfiguration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mhconf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprefix\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.auth.url'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'https://identity.open.softlayer.com'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'/v3/auth/tokens'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mhconf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprefix\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.auth.endpoint.prefix'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'endpoints'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'hadoopConfiguration'"
                    ], 
                    "output_type": "error"
                }
            ]
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "source": "set_hadoop_config_with_credentials(credentials_1) \nfileNameOut = 'swift://'+ credentials_1['container'] + '.keystone/dataset_analise_sentimento.csv' \narquivo = sc.textFile(fileNameOut)", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "#Frequencia de update em segundos\nINTERVALO_BATCH = 5", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "source": "#Criando o Streaming Context\nscc = StreamingContext(sc,INTERVALO_BATCH)", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "source": "arquivo.take(5)", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "source": "#Removendo o cabe\u00e7alho\nheader = arquivo.take(1)[0]\ndataset = arquivo.filter(lambda line : line != header)\n", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "source": "type(dataset)", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "source": "# Essa fun\u00e7\u00e3o separa as colunas em cada linha, cria uma tupla e remove a pontua\u00e7\u00e3o.\ndef get_row(line):\n  row = line.split(',')\n  sentimento = row[1]\n  tweet = row[3].strip()\n  translator = str.maketrans({key: None for key in string.punctuation})\n#translator = re.compile('[%s]' % re.escape(string.punctuation))\n#tweet = regex.sub('', tweet)\n  tweet = tweet.translate(translator)\n  tweet = tweet.split(' ')\n  tweet_lower = []\n  for word in tweet:\n    tweet_lower.append(word.lower())\n  return (tweet_lower, sentimento)", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "source": "# Aplcia a fun\u00e7\u00e3o a cada linha do dataset\ndataset_treino = dataset.map(lambda line: get_row(line))", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "# Cria um objeto SentimentAnalyzer \nsentiment_analyzer = SentimentAnalyzer()", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "source": "#Certifique se de ter espa\u00e7o em disco - Aproximadamente 5GB\nnltk.download(\"stopwords\")", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "#Stopwords... palavras que nao devem ser processadas\nstopwords_all = []", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "source": "for word in stopwords.words(\"english\"):\n    stopwords_all.append(word)\n    stopwords_all.append(word + \"_NEG\")", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "#Obtem 10000 tweets do dataset de treino e retorna as palavras que nao sao stopwords\ndataset_treino_amostra = dataset_treino.take(10000)\n\nall_words_neg = sentiment_analyzer.all_words([mark_negation(doc) for doc in dataset_treino_amostra])\nall_words_neg_nostops = [x for x in all_words_neg if x not in stopwords_all]", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "#Cria um unigram e extrai as features\nunigram_feats = sentiment_analyzer.unigram_word_feats(all_words_neg_nostops,top_n=200)\nsentiment_analyzer.add_feat_extractor(extract_unigram_feats,unigrams = unigram_feats)\ntraining_set = sentiment_analyzer.apply_features(dataset_treino_amostra)", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "source": "type(training_set)", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "source": "#Treinar o modelo\ntrainer = NaiveBayesClassifier.train\nclassifier = sentiment_analyzer.train(trainer=trainer,training_set=training_set)", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "#Testa o classificador em algumas senten\u00e7as\ntest_sentence1 = [([\"this\",\"program\",\"is\",\"bad\"],\"\")]\ntest_set = sentiment_analyzer.apply_features(test_sentence1)", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "source": "#Autenticacao do Twitter\nconsumer_key=\"bxAZviP8aBu45zmqzACuSdV5A\"\nconsumer_secret=\"OqnhD4qAB1nsi6V2AgHZlinJYs3WupPMgfB8pixgWUB3DNfToX\"\naccess_token=\"148351945-8wpWXpGzgY0SM5OHCEXQwlfSCrPg5kH5tnQEgiag\"\naccess_token_secret=\"ITtY8GC5Bv63CxaobchBEHi5XHsERxaCOqAqH1NjGoxgi\"", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "#Especifica a URL termo de busca\nsearch_term = \"Dilma\"\nsample_url = \"https://stream.twitter.com/1.1/statuses/sample.json\"\nfilter_url = \"https://stream.twitter.com/1.1/statuses/filter.json?track=\" + search_term", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "#Criando o objeto de autenticacao do twitter\nauth = requests_oauthlib.OAuth1(consumer_key,consumer_secret,access_token,access_token_secret)", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "source": "#Configurando o Stream\nrdd = scc.sparkContext.parallelize([0])\nstream = scc.queueStream([],default=rdd)\ntype(stream)", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "# Total de tweets por update\nNUM_TWEETS = 500  ", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "# Essa fun\u00e7\u00e3o conecta ao Twitter e retorna um n\u00famero espec\u00edfico de Tweets (NUM_TWEETS)\ndef tfunc(t, rdd):\n  return rdd.flatMap(lambda x: stream_twitter_data())\n\ndef stream_twitter_data():\n  response = requests.get(filter_url, auth = auth, stream = True)\n  print(filter_url, response)\n  count = 0\n  for line in response.iter_lines():\n    try:\n      if count > NUM_TWEETS:\n        break\n      post = json.loads(line.decode('utf-8'))\n      contents = [post['text']]\n      count += 1\n      yield str(contents)\n    except:\n      result = False", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "stream = stream.transform(tfunc)", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "coord_stream = stream.map(lambda line: ast.literal_eval(line))", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "# Essa fun\u00e7\u00e3o classifica os tweets, aplicando as features do modelo criado anteriormente\ndef classifica_tweet(tweet):\n  sentence = [(tweet, '')]\n  test_set = sentiment_analyzer.apply_features(sentence)\n  print(tweet, classifier.classify(test_set[0][0]))\n  return(tweet, classifier.classify(test_set[0][0]))", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "# Essa fun\u00e7\u00e3o retorna o texto do Twitter\ndef get_tweet_text(rdd):\n  for line in rdd:\n    tweet = line.strip()\n    translator = str.maketrans({key: None for key in string.punctuation})\n    tweet = tweet.translate(translator)\n    tweet = tweet.split(' ')\n    tweet_lower = []\n    for word in tweet:\n      tweet_lower.append(word.lower())\n    return(classifica_tweet(tweet_lower))", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "# Cria uma lista vazia para os resultados\nresultados = []", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "# Essa fun\u00e7\u00e3o salva o resultado dos batches de Tweets junto com o timestamp\ndef output_rdd(rdd):\n  global resultados\n  pairs = rdd.map(lambda x: (get_tweet_text(x)[1],1))\n  counts = pairs.reduceByKey(add)\n  output = []\n  for count in counts.collect():\n    output.append(count)\n  result = [time.strftime(\"%I:%M:%S\"), output]\n  resultados.append(result)\n  print(result)", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "# A fun\u00e7\u00e3o foreachRDD() aplica uma fun\u00e7\u00e3o a cada RDD to streaming de dados\ncoord_stream.foreachRDD(lambda t, rdd: output_rdd(rdd))", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "source": "# Start streaming\nscc.start()\n# ssc.awaitTermination()", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "#scc.stop()", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }
    ], 
    "nbformat": 4
}