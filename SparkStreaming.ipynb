{
    "cells": [
        {
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }, 
            "source": "# Spark Streaming com Twitter"
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [
                {
                    "text": "Requirement already satisfied (use --upgrade to upgrade): requests_oauthlib in /gpfs/global_fs01/sym_shared/YPProdSpark/user/sf2f-8f4bf40bb9b7e1-6bd8badf036a/.local/lib/python3.5/site-packages\nRequirement already satisfied (use --upgrade to upgrade): requests>=2.0.0 in /usr/local/src/conda3_runtime.v9/4.1.1/lib/python3.5/site-packages (from requests_oauthlib)\nRequirement already satisfied (use --upgrade to upgrade): oauthlib>=0.6.2 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/sf2f-8f4bf40bb9b7e1-6bd8badf036a/.local/lib/python3.5/site-packages (from requests_oauthlib)\n", 
                    "name": "stdout", 
                    "output_type": "stream"
                }
            ], 
            "source": "!pip install requests_oauthlib", 
            "execution_count": 6
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "from pyspark.streaming import StreamingContext\nfrom pyspark import SparkContext\nfrom requests_oauthlib import OAuth1Session\nfrom operator import add\nimport requests_oauthlib\nfrom time import gmtime,strftime\nimport requests\nimport time\nimport string\nimport ast\nimport json", 
            "execution_count": 7
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [], 
            "source": "#Pacote NLTK\nimport nltk\nfrom nltk.classify import NaiveBayesClassifier\nfrom nltk.sentiment import SentimentAnalyzer\nfrom nltk.corpus import subjectivity\nfrom nltk.corpus import stopwords\nfrom nltk.sentiment.util import *", 
            "execution_count": 8
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [], 
            "source": "\n# @hidden_cell\ncredentials_1 = {\n  'auth_url':'https://identity.open.softlayer.com',\n  'project':'object_storage_f0d6ce32_5e0f_4bc0_8812_229b8d429dbe',\n  'project_id':'9a0cc60102244d368e96a83f25d4ca89',\n  'region':'dallas',\n  'user_id':'0caf8026c98a4342ac027a05416e6dee',\n  'domain_id':'3be46074545f4c09b1f10df3ace95998',\n  'domain_name':'1351407',\n  'username':'member_327b95c3eecf105b8bdb0125b81968cfcc557dbd',\n  'password':\"\"\"D[Cvr1bgf9DM^I{C\"\"\",\n  'container':'CursoSpark',\n  'tenantId':'undefined',\n  'filename':'dataset_analise_sentimento.csv'\n}\n", 
            "execution_count": 29
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [], 
            "source": "def set_hadoop_config_with_credentials(creds):  \n    \"\"\"This function sets the Hadoop configuration so it is possible to\n    access data from Bluemix Object Storage using Spark\"\"\"\n\n    # you can choose any name\n    name = 'keystone'\n\n    prefix = 'fs.swift.service.' + name\n    hconf = sc._jsc.hadoopConfiguration()\n    hconf.set(prefix + '.auth.url', 'https://identity.open.softlayer.com'+'/v3/auth/tokens')\n    hconf.set(prefix + '.auth.endpoint.prefix', 'endpoints')\n    hconf.set(prefix + '.tenant', creds['project_id'])\n    hconf.set(prefix + '.username', creds['user_id'])\n    hconf.set(prefix + '.password', creds['password'])\n    hconf.setInt(prefix + '.http.port', 8080)\n    hconf.set(prefix + '.region', 'dallas')\n    hconf.setBoolean(prefix + '.public', False)", 
            "execution_count": 37
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "\nfrom pyspark.sql import SparkSession\n\n# @hidden_cell\n# This function is used to setup the access of Spark to your Object Storage. The definition contains your credentials.\n# You might want to remove those credentials before you share your notebook.\ndef set_hadoop_config_with_credentials_f0d6ce325e0f4bc08812229b8d429dbe(name):\n    \"\"\"This function sets the Hadoop configuration so it is possible to\n    access data from Bluemix Object Storage using Spark\"\"\"\n\n    prefix = 'fs.swift.service.' + name\n    hconf = sc._jsc.hadoopConfiguration()\n    hconf.set(prefix + '.auth.url', 'https://identity.open.softlayer.com'+'/v3/auth/tokens')\n    hconf.set(prefix + '.auth.endpoint.prefix', 'endpoints')\n    hconf.set(prefix + '.tenant', '9a0cc60102244d368e96a83f25d4ca89')\n    hconf.set(prefix + '.username', '0caf8026c98a4342ac027a05416e6dee')\n    hconf.set(prefix + '.password', 'D[Cvr1bgf9DM^I{C')\n    hconf.setInt(prefix + '.http.port', 8080)\n    hconf.set(prefix + '.region', 'dallas')\n    hconf.setBoolean(prefix + '.public', False)\n\n# you can choose any name\nname = 'keystone'\nset_hadoop_config_with_credentials_f0d6ce325e0f4bc08812229b8d429dbe(name)\n\nspark = SparkSession.builder.getOrCreate()\n\ndf_data_1 = spark.read\\\n  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n  .option('header', 'true')\\\n  .load('swift://CursoSpark.' + name + '/dataset_analise_sentimento.csv')\n", 
            "execution_count": 36
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [], 
            "source": "set_hadoop_config_with_credentials(credentials_1) \nfileNameOut = 'swift://'+ credentials_1['container'] + '.keystone/dataset_analise_sentimento.csv' \narquivo = sc.textFile(fileNameOut)", 
            "execution_count": 40
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "", 
            "execution_count": null
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "#Frequencia de update em segundos\nINTERVALO_BATCH = 5", 
            "execution_count": 10
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [], 
            "source": "#Criando o Streaming Context\nscc = StreamingContext(sc,INTERVALO_BATCH)", 
            "execution_count": 11
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [
                {
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "['ItemID,Sentiment,SentimentSource,SentimentText',\n '1,0,Sentiment140,                     is so sad for my APL friend.............',\n '2,0,Sentiment140,                   I missed the New Moon trailer...',\n '3,1,Sentiment140,              omg its already 7:30 :O',\n \"4,0,Sentiment140,          .. Omgaga. Im sooo  im gunna CRy. I've been at this dentist since 11.. I was suposed 2 just get a crown put on (30mins)...\"]"
                    }, 
                    "execution_count": 41
                }
            ], 
            "source": "arquivo.take(5)", 
            "execution_count": 41
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [], 
            "source": "#Removendo o cabe\u00e7alho\nheader = arquivo.take(1)[0]\ndataset = arquivo.filter(lambda line : line != header)\n", 
            "execution_count": 45
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [
                {
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "pyspark.rdd.PipelinedRDD"
                    }, 
                    "execution_count": 46
                }
            ], 
            "source": "type(dataset)", 
            "execution_count": 46
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [], 
            "source": "# Essa fun\u00e7\u00e3o separa as colunas em cada linha, cria uma tupla e remove a pontua\u00e7\u00e3o.\ndef get_row(line):\n  row = line.split(',')\n  sentimento = row[1]\n  tweet = row[3].strip()\n  translator = str.maketrans({key: None for key in string.punctuation})\n#translator = re.compile('[%s]' % re.escape(string.punctuation))\n#tweet = regex.sub('', tweet)\n  tweet = tweet.translate(translator)\n  tweet = tweet.split(' ')\n  tweet_lower = []\n  for word in tweet:\n    tweet_lower.append(word.lower())\n  return (tweet_lower, sentimento)", 
            "execution_count": 47
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [], 
            "source": "# Aplcia a fun\u00e7\u00e3o a cada linha do dataset\ndataset_treino = dataset.map(lambda line: get_row(line))", 
            "execution_count": 48
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "# Cria um objeto SentimentAnalyzer \nsentiment_analyzer = SentimentAnalyzer()", 
            "execution_count": 49
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [
                {
                    "text": "[nltk_data] Downloading package stopwords to /gpfs/fs01/user/sf2f-8f4b\n[nltk_data]     f40bb9b7e1-6bd8badf036a/nltk_data...\n[nltk_data]   Unzipping corpora/stopwords.zip.\n", 
                    "name": "stdout", 
                    "output_type": "stream"
                }, 
                {
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "True"
                    }, 
                    "execution_count": 50
                }
            ], 
            "source": "#Certifique se de ter espa\u00e7o em disco - Aproximadamente 5GB\nnltk.download(\"stopwords\")", 
            "execution_count": 50
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "", 
            "execution_count": null
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5 (Experimental) with Spark 2.0", 
            "name": "python3-spark20", 
            "language": "python"
        }, 
        "language_info": {
            "version": "3.5.2", 
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }, 
            "file_extension": ".py", 
            "name": "python"
        }
    }, 
    "nbformat_minor": 0, 
    "nbformat": 4
}