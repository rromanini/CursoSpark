{
    "nbformat_minor": 0, 
    "metadata": {
        "language_info": {
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "name": "ipython", 
                "version": 3
            }, 
            "file_extension": ".py", 
            "version": "3.5.2", 
            "mimetype": "text/x-python", 
            "name": "python", 
            "nbconvert_exporter": "python"
        }, 
        "kernelspec": {
            "name": "python3-spark20", 
            "language": "python", 
            "display_name": "Python 3.5 (Experimental) with Spark 2.0"
        }
    }, 
    "cells": [
        {
            "source": "# Spark Streaming com Twitter", 
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "source": "!pip install requests_oauthlib", 
            "execution_count": 6, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "text": "Requirement already satisfied (use --upgrade to upgrade): requests_oauthlib in /gpfs/global_fs01/sym_shared/YPProdSpark/user/sf2f-8f4bf40bb9b7e1-6bd8badf036a/.local/lib/python3.5/site-packages\nRequirement already satisfied (use --upgrade to upgrade): requests>=2.0.0 in /usr/local/src/conda3_runtime.v9/4.1.1/lib/python3.5/site-packages (from requests_oauthlib)\nRequirement already satisfied (use --upgrade to upgrade): oauthlib>=0.6.2 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/sf2f-8f4bf40bb9b7e1-6bd8badf036a/.local/lib/python3.5/site-packages (from requests_oauthlib)\n", 
                    "output_type": "stream"
                }
            ]
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "from pyspark.streaming import StreamingContext\nfrom pyspark import SparkContext\nfrom requests_oauthlib import OAuth1Session\nfrom operator import add\nimport requests_oauthlib\nfrom time import gmtime,strftime\nimport requests\nimport time\nimport string\nimport ast\nimport json", 
            "execution_count": 7, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "source": "#Pacote NLTK\nimport nltk\nfrom nltk.classify import NaiveBayesClassifier\nfrom nltk.sentiment import SentimentAnalyzer\nfrom nltk.corpus import subjectivity\nfrom nltk.corpus import stopwords\nfrom nltk.sentiment.util import *", 
            "execution_count": 8, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "source": "\n# @hidden_cell\ncredentials_1 = {\n  'auth_url':'https://identity.open.softlayer.com',\n  'project':'object_storage_f0d6ce32_5e0f_4bc0_8812_229b8d429dbe',\n  'project_id':'9a0cc60102244d368e96a83f25d4ca89',\n  'region':'dallas',\n  'user_id':'0caf8026c98a4342ac027a05416e6dee',\n  'domain_id':'3be46074545f4c09b1f10df3ace95998',\n  'domain_name':'1351407',\n  'username':'member_327b95c3eecf105b8bdb0125b81968cfcc557dbd',\n  'password':\"\"\"D[Cvr1bgf9DM^I{C\"\"\",\n  'container':'CursoSpark',\n  'tenantId':'undefined',\n  'filename':'dataset_analise_sentimento.csv'\n}\n", 
            "execution_count": 29, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "source": "def set_hadoop_config_with_credentials(creds):  \n    \"\"\"This function sets the Hadoop configuration so it is possible to\n    access data from Bluemix Object Storage using Spark\"\"\"\n\n    # you can choose any name\n    name = 'keystone'\n\n    prefix = 'fs.swift.service.' + name\n    hconf = sc._jsc.hadoopConfiguration()\n    hconf.set(prefix + '.auth.url', 'https://identity.open.softlayer.com'+'/v3/auth/tokens')\n    hconf.set(prefix + '.auth.endpoint.prefix', 'endpoints')\n    hconf.set(prefix + '.tenant', creds['project_id'])\n    hconf.set(prefix + '.username', creds['user_id'])\n    hconf.set(prefix + '.password', creds['password'])\n    hconf.setInt(prefix + '.http.port', 8080)\n    hconf.set(prefix + '.region', 'dallas')\n    hconf.setBoolean(prefix + '.public', False)", 
            "execution_count": 37, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "\nfrom pyspark.sql import SparkSession\n\n# @hidden_cell\n# This function is used to setup the access of Spark to your Object Storage. The definition contains your credentials.\n# You might want to remove those credentials before you share your notebook.\ndef set_hadoop_config_with_credentials_f0d6ce325e0f4bc08812229b8d429dbe(name):\n    \"\"\"This function sets the Hadoop configuration so it is possible to\n    access data from Bluemix Object Storage using Spark\"\"\"\n\n    prefix = 'fs.swift.service.' + name\n    hconf = sc._jsc.hadoopConfiguration()\n    hconf.set(prefix + '.auth.url', 'https://identity.open.softlayer.com'+'/v3/auth/tokens')\n    hconf.set(prefix + '.auth.endpoint.prefix', 'endpoints')\n    hconf.set(prefix + '.tenant', '9a0cc60102244d368e96a83f25d4ca89')\n    hconf.set(prefix + '.username', '0caf8026c98a4342ac027a05416e6dee')\n    hconf.set(prefix + '.password', 'D[Cvr1bgf9DM^I{C')\n    hconf.setInt(prefix + '.http.port', 8080)\n    hconf.set(prefix + '.region', 'dallas')\n    hconf.setBoolean(prefix + '.public', False)\n\n# you can choose any name\nname = 'keystone'\nset_hadoop_config_with_credentials_f0d6ce325e0f4bc08812229b8d429dbe(name)\n\nspark = SparkSession.builder.getOrCreate()\n\ndf_data_1 = spark.read\\\n  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n  .option('header', 'true')\\\n  .load('swift://CursoSpark.' + name + '/dataset_analise_sentimento.csv')\n", 
            "execution_count": 36, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "source": "set_hadoop_config_with_credentials(credentials_1) \nfileNameOut = 'swift://'+ credentials_1['container'] + '.keystone/dataset_analise_sentimento.csv' \narquivo = sc.textFile(fileNameOut)", 
            "execution_count": 40, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "#Frequencia de update em segundos\nINTERVALO_BATCH = 5", 
            "execution_count": 10, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "source": "#Criando o Streaming Context\nscc = StreamingContext(sc,INTERVALO_BATCH)", 
            "execution_count": 68, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "source": "arquivo.take(5)", 
            "execution_count": 41, 
            "cell_type": "code", 
            "outputs": [
                {
                    "execution_count": 41, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "['ItemID,Sentiment,SentimentSource,SentimentText',\n '1,0,Sentiment140,                     is so sad for my APL friend.............',\n '2,0,Sentiment140,                   I missed the New Moon trailer...',\n '3,1,Sentiment140,              omg its already 7:30 :O',\n \"4,0,Sentiment140,          .. Omgaga. Im sooo  im gunna CRy. I've been at this dentist since 11.. I was suposed 2 just get a crown put on (30mins)...\"]"
                    }, 
                    "metadata": {}
                }
            ]
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "source": "#Removendo o cabe\u00e7alho\nheader = arquivo.take(1)[0]\ndataset = arquivo.filter(lambda line : line != header)\n", 
            "execution_count": 45, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "source": "type(dataset)", 
            "execution_count": 46, 
            "cell_type": "code", 
            "outputs": [
                {
                    "execution_count": 46, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "pyspark.rdd.PipelinedRDD"
                    }, 
                    "metadata": {}
                }
            ]
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "source": "# Essa fun\u00e7\u00e3o separa as colunas em cada linha, cria uma tupla e remove a pontua\u00e7\u00e3o.\ndef get_row(line):\n  row = line.split(',')\n  sentimento = row[1]\n  tweet = row[3].strip()\n  translator = str.maketrans({key: None for key in string.punctuation})\n#translator = re.compile('[%s]' % re.escape(string.punctuation))\n#tweet = regex.sub('', tweet)\n  tweet = tweet.translate(translator)\n  tweet = tweet.split(' ')\n  tweet_lower = []\n  for word in tweet:\n    tweet_lower.append(word.lower())\n  return (tweet_lower, sentimento)", 
            "execution_count": 47, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "source": "# Aplcia a fun\u00e7\u00e3o a cada linha do dataset\ndataset_treino = dataset.map(lambda line: get_row(line))", 
            "execution_count": 48, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "# Cria um objeto SentimentAnalyzer \nsentiment_analyzer = SentimentAnalyzer()", 
            "execution_count": 49, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "source": "#Certifique se de ter espa\u00e7o em disco - Aproximadamente 5GB\nnltk.download(\"stopwords\")", 
            "execution_count": 50, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "text": "[nltk_data] Downloading package stopwords to /gpfs/fs01/user/sf2f-8f4b\n[nltk_data]     f40bb9b7e1-6bd8badf036a/nltk_data...\n[nltk_data]   Unzipping corpora/stopwords.zip.\n", 
                    "output_type": "stream"
                }, 
                {
                    "execution_count": 50, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "True"
                    }, 
                    "metadata": {}
                }
            ]
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "#Stopwords... palavras que nao devem ser processadas\nstopwords_all = []", 
            "execution_count": 51, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "source": "for word in stopwords.words(\"english\"):\n    stopwords_all.append(word)\n    stopwords_all.append(word + \"_NEG\")", 
            "execution_count": 54, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "#Obtem 10000 tweets do dataset de treino e retorna as palavras que nao sao stopwords\ndataset_treino_amostra = dataset_treino.take(10000)\n\nall_words_neg = sentiment_analyzer.all_words([mark_negation(doc) for doc in dataset_treino_amostra])\nall_words_neg_nostops = [x for x in all_words_neg if x not in stopwords_all]", 
            "execution_count": 57, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "#Cria um unigram e extrai as features\nunigram_feats = sentiment_analyzer.unigram_word_feats(all_words_neg_nostops,top_n=200)\nsentiment_analyzer.add_feat_extractor(extract_unigram_feats,unigrams = unigram_feats)\ntraining_set = sentiment_analyzer.apply_features(dataset_treino_amostra)", 
            "execution_count": 60, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "source": "type(training_set)", 
            "execution_count": 61, 
            "cell_type": "code", 
            "outputs": [
                {
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "nltk.util.LazyMap"
                    }, 
                    "execution_count": 61
                }
            ]
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "source": "#Treinar o modelo\ntrainer = NaiveBayesClassifier.train\nclassifier = sentiment_analyzer.train(trainer=trainer,training_set=training_set)", 
            "execution_count": 62, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "text": "Training classifier\n", 
                    "output_type": "stream"
                }
            ]
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "#Testa o classificador em algumas senten\u00e7as\ntest_sentence1 = [([\"this\",\"program\",\"is\",\"bad\"],\"\")]\ntest_set = sentiment_analyzer.apply_features(test_sentence1)", 
            "execution_count": 63, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "source": "#Autenticacao do Twitter\nconsumer_key=\"\"\nconsumer_secret=\"\"\naccess_token=\"\"\naccess_token_secret=\"\"", 
            "execution_count": 65, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "#Especifica a URL termo de busca\nsearch_term = \"Trump\"\nsample_url = \"https://stream.twitter.com/1.1/statuses/sample.json\"\nfilter_url = \"https://stream.twitter.com/1.1/statuses/filter.json?track=\" + search_term", 
            "execution_count": 66, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "#Criando o objeto de autenticacao do twitter\nauth = requests_oauthlib.OAuth1(consumer_key,consumer_secret,access_token,access_token_secret)", 
            "execution_count": 67, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "source": "#Configurando o Stream\nrdd = scc.sparkContext.parallelize([0])\nstream = scc.queueStream([],default=rdd)\ntype(stream)", 
            "execution_count": 71, 
            "cell_type": "code", 
            "outputs": [
                {
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "pyspark.streaming.dstream.DStream"
                    }, 
                    "execution_count": 71
                }
            ]
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }
    ], 
    "nbformat": 4
}