{
    "nbformat_minor": 0, 
    "nbformat": 4, 
    "cells": [
        {
            "source": "# Spark Streaming com Twitter", 
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "source": "!pip install requests_oauthlib", 
            "execution_count": 4, 
            "outputs": [
                {
                    "name": "stdout", 
                    "text": "Collecting requests_oauthlib\n  Downloading requests_oauthlib-0.8.0-py2.py3-none-any.whl\nRequirement already satisfied (use --upgrade to upgrade): requests>=2.0.0 in /usr/local/src/conda3_runtime.v9/4.1.1/lib/python3.5/site-packages (from requests_oauthlib)\nCollecting oauthlib>=0.6.2 (from requests_oauthlib)\n  Downloading oauthlib-2.0.2.tar.gz (125kB)\n\u001b[K    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 133kB 4.8MB/s \n\u001b[?25hBuilding wheels for collected packages: oauthlib\n  Running setup.py bdist_wheel for oauthlib ... \u001b[?25l-\b \b\\\b \bdone\n\u001b[?25h  Stored in directory: /gpfs/fs01/user/sf2f-8f4bf40bb9b7e1-6bd8badf036a/.cache/pip/wheels/84/98/7a/fba7268f61097bea6081cbe5480bc439b38975748ea7684fd5\nSuccessfully built oauthlib\nInstalling collected packages: oauthlib, requests-oauthlib\nSuccessfully installed oauthlib-2.0.2 requests-oauthlib-0.8.0\n", 
                    "output_type": "stream"
                }
            ], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "from pyspark.streaming import StreamingContext\nfrom pyspark import SparkContext\nfrom requests_oauthlib import OAuth1Session\nfrom operator import add\nimport requests_oauthlib\nfrom time import gmtime,strftime\nimport requests\nimport time\nimport string\nimport ast\nimport json", 
            "execution_count": 10, 
            "outputs": [], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "source": "#Pacote NLTK\nimport nltk\nfrom nltk.classify import NaiveBayesClassifier\nfrom nltk.sentiment import SentimentAnalyzer\nfrom nltk.corpus import subjectivity\nfrom nltk.corpus import stopwords\nfrom nltk.sentiment.util import *", 
            "execution_count": 17, 
            "outputs": [], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "source": "#Frequencia de update em segundos\nINTERVALO_BATCH = 5", 
            "execution_count": 18, 
            "outputs": [], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "source": "#Criando o Streaming Context\nscc = StreamingContext(sc,INTERVALO_BATCH)", 
            "execution_count": 19, 
            "outputs": [], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "source": "#Removendo o cabe\u00e7alho\nheader = arquivo.take(1)[0]\ndataset = arquivo.filter(lambda line : line != header)\n", 
            "execution_count": 20, 
            "outputs": [
                {
                    "ename": "NameError", 
                    "evalue": "name 'arquivo' is not defined", 
                    "traceback": [
                        "\u001b[1;31m\u001b[0m", 
                        "\u001b[1;31mNameError\u001b[0mTraceback (most recent call last)", 
                        "\u001b[1;32m<ipython-input-20-1b684159b157>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Removendo o cabe\u00e7alho\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mheader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marquivo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marquivo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mline\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mline\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;31mNameError\u001b[0m: name 'arquivo' is not defined"
                    ], 
                    "output_type": "error"
                }
            ], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "type(dataset)", 
            "execution_count": null, 
            "outputs": [], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "source": "# Essa fun\u00e7\u00e3o separa as colunas em cada linha, cria uma tupla e remove a pontua\u00e7\u00e3o.\ndef get_row(line):\n  row = line.split(',')\n  sentimento = row[1]\n  tweet = row[3].strip()\n  translator = str.maketrans({key: None for key in string.punctuation})\n#translator = re.compile('[%s]' % re.escape(string.punctuation))\n#tweet = regex.sub('', tweet)\n  tweet = tweet.translate(translator)\n  tweet = tweet.split(' ')\n  tweet_lower = []\n  for word in tweet:\n    tweet_lower.append(word.lower())\n  return (tweet_lower, sentimento)", 
            "execution_count": 21, 
            "outputs": [
                {
                    "ename": "SyntaxError", 
                    "evalue": "unexpected EOF while parsing (<ipython-input-21-1bf5fdee032f>, line 2)", 
                    "traceback": [
                        "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-21-1bf5fdee032f>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    def get_row(line):\u001b[0m\n\u001b[1;37m                      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
                    ], 
                    "output_type": "error"
                }
            ], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "# Aplcia a fun\u00e7\u00e3o a cada linha do dataset\ndataset_treino = dataset.map(lambda line: get_row(line))", 
            "execution_count": null, 
            "outputs": [], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "source": "# Cria um objeto SentimentAnalyzer \nsentiment_analyzer = SentimentAnalyzer()", 
            "execution_count": null, 
            "outputs": [], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }
        }
    ], 
    "metadata": {
        "kernelspec": {
            "name": "python3-spark20", 
            "language": "python", 
            "display_name": "Python 3.5 (Experimental) with Spark 2.0"
        }, 
        "language_info": {
            "name": "python", 
            "pygments_lexer": "ipython3", 
            "nbconvert_exporter": "python", 
            "mimetype": "text/x-python", 
            "file_extension": ".py", 
            "codemirror_mode": {
                "name": "ipython", 
                "version": 3
            }, 
            "version": "3.5.2"
        }
    }
}