{
    "nbformat_minor": 0, 
    "cells": [
        {
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }, 
            "source": "# Spark Streaming com Twitter"
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "Requirement already satisfied (use --upgrade to upgrade): requests_oauthlib in /gpfs/global_fs01/sym_shared/YPProdSpark/user/sf2f-8f4bf40bb9b7e1-6bd8badf036a/.local/lib/python3.5/site-packages\nRequirement already satisfied (use --upgrade to upgrade): requests>=2.0.0 in /usr/local/src/conda3_runtime.v9/4.1.1/lib/python3.5/site-packages (from requests_oauthlib)\nRequirement already satisfied (use --upgrade to upgrade): oauthlib>=0.6.2 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/sf2f-8f4bf40bb9b7e1-6bd8badf036a/.local/lib/python3.5/site-packages (from requests_oauthlib)\n"
                }
            ], 
            "source": "!pip install requests_oauthlib", 
            "execution_count": 6
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "from pyspark.streaming import StreamingContext\nfrom pyspark import SparkContext\nfrom requests_oauthlib import OAuth1Session\nfrom operator import add\nimport requests_oauthlib\nfrom time import gmtime,strftime\nimport requests\nimport time\nimport string\nimport ast\nimport json", 
            "execution_count": 7
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [], 
            "source": "#Pacote NLTK\nimport nltk\nfrom nltk.classify import NaiveBayesClassifier\nfrom nltk.sentiment import SentimentAnalyzer\nfrom nltk.corpus import subjectivity\nfrom nltk.corpus import stopwords\nfrom nltk.sentiment.util import *", 
            "execution_count": 8
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [], 
            "source": "\n# @hidden_cell\ncredentials_1 = {\n  'auth_url':'https://identity.open.softlayer.com',\n  'project':'object_storage_f0d6ce32_5e0f_4bc0_8812_229b8d429dbe',\n  'project_id':'9a0cc60102244d368e96a83f25d4ca89',\n  'region':'dallas',\n  'user_id':'0caf8026c98a4342ac027a05416e6dee',\n  'domain_id':'3be46074545f4c09b1f10df3ace95998',\n  'domain_name':'1351407',\n  'username':'member_327b95c3eecf105b8bdb0125b81968cfcc557dbd',\n  'password':\"\"\"D[Cvr1bgf9DM^I{C\"\"\",\n  'container':'CursoSpark',\n  'tenantId':'undefined',\n  'filename':'dataset_analise_sentimento.csv'\n}\n", 
            "execution_count": 29
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [], 
            "source": "def set_hadoop_config_with_credentials(creds):  \n    \"\"\"This function sets the Hadoop configuration so it is possible to\n    access data from Bluemix Object Storage using Spark\"\"\"\n\n    # you can choose any name\n    name = 'keystone'\n\n    prefix = 'fs.swift.service.' + name\n    hconf = sc._jsc.hadoopConfiguration()\n    hconf.set(prefix + '.auth.url', 'https://identity.open.softlayer.com'+'/v3/auth/tokens')\n    hconf.set(prefix + '.auth.endpoint.prefix', 'endpoints')\n    hconf.set(prefix + '.tenant', creds['project_id'])\n    hconf.set(prefix + '.username', creds['user_id'])\n    hconf.set(prefix + '.password', creds['password'])\n    hconf.setInt(prefix + '.http.port', 8080)\n    hconf.set(prefix + '.region', 'dallas')\n    hconf.setBoolean(prefix + '.public', False)", 
            "execution_count": 37
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "\nfrom pyspark.sql import SparkSession\n\n# @hidden_cell\n# This function is used to setup the access of Spark to your Object Storage. The definition contains your credentials.\n# You might want to remove those credentials before you share your notebook.\ndef set_hadoop_config_with_credentials_f0d6ce325e0f4bc08812229b8d429dbe(name):\n    \"\"\"This function sets the Hadoop configuration so it is possible to\n    access data from Bluemix Object Storage using Spark\"\"\"\n\n    prefix = 'fs.swift.service.' + name\n    hconf = sc._jsc.hadoopConfiguration()\n    hconf.set(prefix + '.auth.url', 'https://identity.open.softlayer.com'+'/v3/auth/tokens')\n    hconf.set(prefix + '.auth.endpoint.prefix', 'endpoints')\n    hconf.set(prefix + '.tenant', '9a0cc60102244d368e96a83f25d4ca89')\n    hconf.set(prefix + '.username', '0caf8026c98a4342ac027a05416e6dee')\n    hconf.set(prefix + '.password', 'D[Cvr1bgf9DM^I{C')\n    hconf.setInt(prefix + '.http.port', 8080)\n    hconf.set(prefix + '.region', 'dallas')\n    hconf.setBoolean(prefix + '.public', False)\n\n# you can choose any name\nname = 'keystone'\nset_hadoop_config_with_credentials_f0d6ce325e0f4bc08812229b8d429dbe(name)\n\nspark = SparkSession.builder.getOrCreate()\n\ndf_data_1 = spark.read\\\n  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n  .option('header', 'true')\\\n  .load('swift://CursoSpark.' + name + '/dataset_analise_sentimento.csv')\n", 
            "execution_count": 36
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [], 
            "source": "set_hadoop_config_with_credentials(credentials_1) \nfileNameOut = 'swift://'+ credentials_1['container'] + '.keystone/dataset_analise_sentimento.csv' \narquivo = sc.textFile(fileNameOut)", 
            "execution_count": 40
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "", 
            "execution_count": null
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "#Frequencia de update em segundos\nINTERVALO_BATCH = 5", 
            "execution_count": 10
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [], 
            "source": "#Criando o Streaming Context\nscc = StreamingContext(sc,INTERVALO_BATCH)", 
            "execution_count": 68
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "['ItemID,Sentiment,SentimentSource,SentimentText',\n '1,0,Sentiment140,                     is so sad for my APL friend.............',\n '2,0,Sentiment140,                   I missed the New Moon trailer...',\n '3,1,Sentiment140,              omg its already 7:30 :O',\n \"4,0,Sentiment140,          .. Omgaga. Im sooo  im gunna CRy. I've been at this dentist since 11.. I was suposed 2 just get a crown put on (30mins)...\"]"
                    }, 
                    "metadata": {}, 
                    "execution_count": 41
                }
            ], 
            "source": "arquivo.take(5)", 
            "execution_count": 41
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [], 
            "source": "#Removendo o cabe\u00e7alho\nheader = arquivo.take(1)[0]\ndataset = arquivo.filter(lambda line : line != header)\n", 
            "execution_count": 45
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "pyspark.rdd.PipelinedRDD"
                    }, 
                    "metadata": {}, 
                    "execution_count": 46
                }
            ], 
            "source": "type(dataset)", 
            "execution_count": 46
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [], 
            "source": "# Essa fun\u00e7\u00e3o separa as colunas em cada linha, cria uma tupla e remove a pontua\u00e7\u00e3o.\ndef get_row(line):\n  row = line.split(',')\n  sentimento = row[1]\n  tweet = row[3].strip()\n  translator = str.maketrans({key: None for key in string.punctuation})\n#translator = re.compile('[%s]' % re.escape(string.punctuation))\n#tweet = regex.sub('', tweet)\n  tweet = tweet.translate(translator)\n  tweet = tweet.split(' ')\n  tweet_lower = []\n  for word in tweet:\n    tweet_lower.append(word.lower())\n  return (tweet_lower, sentimento)", 
            "execution_count": 47
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [], 
            "source": "# Aplcia a fun\u00e7\u00e3o a cada linha do dataset\ndataset_treino = dataset.map(lambda line: get_row(line))", 
            "execution_count": 48
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "# Cria um objeto SentimentAnalyzer \nsentiment_analyzer = SentimentAnalyzer()", 
            "execution_count": 49
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "[nltk_data] Downloading package stopwords to /gpfs/fs01/user/sf2f-8f4b\n[nltk_data]     f40bb9b7e1-6bd8badf036a/nltk_data...\n[nltk_data]   Unzipping corpora/stopwords.zip.\n"
                }, 
                {
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "True"
                    }, 
                    "metadata": {}, 
                    "execution_count": 50
                }
            ], 
            "source": "#Certifique se de ter espa\u00e7o em disco - Aproximadamente 5GB\nnltk.download(\"stopwords\")", 
            "execution_count": 50
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "#Stopwords... palavras que nao devem ser processadas\nstopwords_all = []", 
            "execution_count": 51
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [], 
            "source": "for word in stopwords.words(\"english\"):\n    stopwords_all.append(word)\n    stopwords_all.append(word + \"_NEG\")", 
            "execution_count": 54
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "#Obtem 10000 tweets do dataset de treino e retorna as palavras que nao sao stopwords\ndataset_treino_amostra = dataset_treino.take(10000)\n\nall_words_neg = sentiment_analyzer.all_words([mark_negation(doc) for doc in dataset_treino_amostra])\nall_words_neg_nostops = [x for x in all_words_neg if x not in stopwords_all]", 
            "execution_count": 57
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "#Cria um unigram e extrai as features\nunigram_feats = sentiment_analyzer.unigram_word_feats(all_words_neg_nostops,top_n=200)\nsentiment_analyzer.add_feat_extractor(extract_unigram_feats,unigrams = unigram_feats)\ntraining_set = sentiment_analyzer.apply_features(dataset_treino_amostra)", 
            "execution_count": 60
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [
                {
                    "data": {
                        "text/plain": "nltk.util.LazyMap"
                    }, 
                    "output_type": "execute_result", 
                    "execution_count": 61, 
                    "metadata": {}
                }
            ], 
            "source": "type(training_set)", 
            "execution_count": 61
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "Training classifier\n"
                }
            ], 
            "source": "#Treinar o modelo\ntrainer = NaiveBayesClassifier.train\nclassifier = sentiment_analyzer.train(trainer=trainer,training_set=training_set)", 
            "execution_count": 62
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "#Testa o classificador em algumas senten\u00e7as\ntest_sentence1 = [([\"this\",\"program\",\"is\",\"bad\"],\"\")]\ntest_set = sentiment_analyzer.apply_features(test_sentence1)", 
            "execution_count": 63
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [], 
            "source": "#Autenticacao do Twitter\nconsumer_key=\"bxAZviP8aBu45zmqzACuSdV5A\"\nconsumer_secret=\"OqnhD4qAB1nsi6V2AgHZlinJYs3WupPMgfB8pixgWUB3DNfToX\"\naccess_token=\"148351945-8wpWXpGzgY0SM5OHCEXQwlfSCrPg5kH5tnQEgiag\"\naccess_token_secret=\"ITtY8GC5Bv63CxaobchBEHi5XHsERxaCOqAqH1NjGoxgi\"", 
            "execution_count": 65
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "#Especifica a URL termo de busca\nsearch_term = \"Trump\"\nsample_url = \"https://stream.twitter.com/1.1/statuses/sample.json\"\nfilter_url = \"https://stream.twitter.com/1.1/statuses/filter.json?track=\" + search_term", 
            "execution_count": 66
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "#Criando o objeto de autenticacao do twitter\nauth = requests_oauthlib.OAuth1(consumer_key,consumer_secret,access_token,access_token_secret)", 
            "execution_count": 67
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [
                {
                    "data": {
                        "text/plain": "pyspark.streaming.dstream.DStream"
                    }, 
                    "output_type": "execute_result", 
                    "execution_count": 71, 
                    "metadata": {}
                }
            ], 
            "source": "#Configurando o Stream\nrdd = scc.sparkContext.parallelize([0])\nstream = scc.queueStream([],default=rdd)\ntype(stream)", 
            "execution_count": 71
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "# Total de tweets por update\nNUM_TWEETS = 500  ", 
            "execution_count": 72
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "# Essa fun\u00e7\u00e3o conecta ao Twitter e retorna um n\u00famero espec\u00edfico de Tweets (NUM_TWEETS)\ndef tfunc(t, rdd):\n  return rdd.flatMap(lambda x: stream_twitter_data())\n\ndef stream_twitter_data():\n  response = requests.get(filter_url, auth = auth, stream = True)\n  print(filter_url, response)\n  count = 0\n  for line in response.iter_lines():\n    try:\n      if count > NUM_TWEETS:\n        break\n      post = json.loads(line.decode('utf-8'))\n      contents = [post['text']]\n      count += 1\n      yield str(contents)\n    except:\n      result = False", 
            "execution_count": 73
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "stream = stream.transform(tfunc)", 
            "execution_count": 74
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "coord_stream = stream.map(lambda line: ast.literal_eval(line))", 
            "execution_count": 75
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "# Essa fun\u00e7\u00e3o classifica os tweets, aplicando as features do modelo criado anteriormente\ndef classifica_tweet(tweet):\n  sentence = [(tweet, '')]\n  test_set = sentiment_analyzer.apply_features(sentence)\n  print(tweet, classifier.classify(test_set[0][0]))\n  return(tweet, classifier.classify(test_set[0][0]))", 
            "execution_count": 76
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "# Essa fun\u00e7\u00e3o retorna o texto do Twitter\ndef get_tweet_text(rdd):\n  for line in rdd:\n    tweet = line.strip()\n    translator = str.maketrans({key: None for key in string.punctuation})\n    tweet = tweet.translate(translator)\n    tweet = tweet.split(' ')\n    tweet_lower = []\n    for word in tweet:\n      tweet_lower.append(word.lower())\n    return(classifica_tweet(tweet_lower))", 
            "execution_count": 77
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "# Cria uma lista vazia para os resultados\nresultados = []", 
            "execution_count": 78
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "# Essa fun\u00e7\u00e3o salva o resultado dos batches de Tweets junto com o timestamp\ndef output_rdd(rdd):\n  global resultados\n  pairs = rdd.map(lambda x: (get_tweet_text(x)[1],1))\n  counts = pairs.reduceByKey(add)\n  output = []\n  for count in counts.collect():\n    output.append(count)\n  result = [time.strftime(\"%I:%M:%S\"), output]\n  resultados.append(result)\n  print(result)", 
            "execution_count": 79
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "# A fun\u00e7\u00e3o foreachRDD() aplica uma fun\u00e7\u00e3o a cada RDD to streaming de dados\ncoord_stream.foreachRDD(lambda t, rdd: output_rdd(rdd))", 
            "execution_count": 80
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "# Start streaming\nssc.start()\n# ssc.awaitTermination()", 
            "execution_count": null
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5 (Experimental) with Spark 2.0", 
            "language": "python", 
            "name": "python3-spark20"
        }, 
        "language_info": {
            "version": "3.5.2", 
            "nbconvert_exporter": "python", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }, 
            "mimetype": "text/x-python", 
            "file_extension": ".py", 
            "name": "python", 
            "pygments_lexer": "ipython3"
        }
    }, 
    "nbformat": 4
}