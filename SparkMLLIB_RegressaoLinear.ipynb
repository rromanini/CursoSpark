{
    "nbformat_minor": 0, 
    "metadata": {
        "anaconda-cloud": {}, 
        "language_info": {
            "codemirror_mode": {
                "name": "ipython", 
                "version": 2
            }, 
            "version": "2.7.11", 
            "file_extension": ".py", 
            "name": "python", 
            "nbconvert_exporter": "python", 
            "mimetype": "text/x-python", 
            "pygments_lexer": "ipython2"
        }, 
        "kernelspec": {
            "name": "python2-spark20", 
            "display_name": "Python 2 with Spark 2.0", 
            "language": "python"
        }
    }, 
    "nbformat": 4, 
    "cells": [
        {
            "metadata": {}, 
            "source": "# <font color='blue'>Data Science Academy Big Data Real-Time Analytics com Python e Spark</font>\n\n# <font color='blue'>Cap\u00edtulo 9</font>", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "## <font color='blue'>Spark MLLib - Regress\u00e3o Linear</font>", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "<strong> Descri\u00e7\u00e3o </strong>\n<ul style=\"list-style-type:square\">\n  <li>M\u00e9todo para avaliar o relacionamento entre vari\u00e1veis.</li>\n  <li>Estima o valor de uma vari\u00e1vel dependente a partir dos valores das vari\u00e1veis independentes.</li>\n  <li>Usado quando as vari\u00e1veis dependente e independente s\u00e3o cont\u00ednuas e possuem alguma correla\u00e7\u00e3o.</li>\n  <li>O R-Square mede qu\u00e3o perto os dados est\u00e3o da linha de regress\u00e3o. O valor do R-Squared ser\u00e1 entre 0 e 1, sendo que quanto maior o valor, melhor.</li>\n  <li>Os dados de entrada e de sa\u00edda s\u00e3o usados na constru\u00e7\u00e3o do modelo. A equa\u00e7\u00e3o linear retorna os valores dos coeficientes.</li>\n  <li>A equa\u00e7\u00e3o linear representa o modelo.</li>\n</ul>", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "<dl>\n  <dt>Vantagens</dt>\n  <dd>- Baixo custo</dd>\n  <dd>- Veloz</dd>\n  <dd>- Excelente para reala\u00e7\u00e3o lineares</dd>\n  <br />\n  <dt>Desvantagens</dt>\n  <dd>- Somente vari\u00e1veis num\u00e9ricas</dd>\n  <dd>- Sens\u00edvel a outliers</dd>\n  <br />\n  <dt>Aplica\u00e7\u00e3o</dt>\n  <dd>- Um dos modelos mais antigos e pode ser usado para resolver diversos problemas</dd>\n</dl>", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "## Usaremos Regress\u00e3o Linear para prever os valores de MPG (Miles Per Gallon)", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "MPG ser\u00e1 a vari\u00e1vel target e as demais vari\u00e1veis ser\u00e3o as features (vari\u00e1veis preditoras).", 
            "cell_type": "markdown"
        }, 
        {
            "source": "# Spark Session - usada quando se trabalha com Dataframes no Spark\nspark = SparkSession.builder.getOrCreate()", 
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "execution_count": 28, 
            "outputs": []
        }, 
        {
            "source": "from pyspark.sql import Row\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.ml.regression import LinearRegression\nfrom pyspark.ml.evaluation import RegressionEvaluator", 
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "execution_count": 29, 
            "outputs": []
        }, 
        {
            "source": "\n# @hidden_cell\ncredentials_1 = {\n  'auth_url':'https://identity.open.softlayer.com',\n  'project':'object_storage_f0d6ce32_5e0f_4bc0_8812_229b8d429dbe',\n  'project_id':'9a0cc60102244d368e96a83f25d4ca89',\n  'region':'dallas',\n  'user_id':'0caf8026c98a4342ac027a05416e6dee',\n  'domain_id':'3be46074545f4c09b1f10df3ace95998',\n  'domain_name':'1351407',\n  'username':'member_327b95c3eecf105b8bdb0125b81968cfcc557dbd',\n  'password':\"\"\"D[Cvr1bgf9DM^I{C\"\"\",\n  'container':'CursoSpark',\n  'tenantId':'undefined',\n  'filename':'carros.csv'\n}\n", 
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "execution_count": 33, 
            "outputs": []
        }, 
        {
            "source": "\nfrom pyspark.sql import SparkSession\n\n# @hidden_cell\n# This function is used to setup the access of Spark to your Object Storage. The definition contains your credentials.\n# You might want to remove those credentials before you share your notebook.\ndef set_hadoop_config_with_credentials_f0d6ce325e0f4bc08812229b8d429dbe(name):\n    \"\"\"This function sets the Hadoop configuration so it is possible to\n    access data from Bluemix Object Storage using Spark\"\"\"\n\n    prefix = 'fs.swift.service.' + name\n    hconf = sc._jsc.hadoopConfiguration()\n    hconf.set(prefix + '.auth.url', 'https://identity.open.softlayer.com'+'/v3/auth/tokens')\n    hconf.set(prefix + '.auth.endpoint.prefix', 'endpoints')\n    hconf.set(prefix + '.tenant', '9a0cc60102244d368e96a83f25d4ca89')\n    hconf.set(prefix + '.username', '0caf8026c98a4342ac027a05416e6dee')\n    hconf.set(prefix + '.password', 'D[Cvr1bgf9DM^I{C')\n    hconf.setInt(prefix + '.http.port', 8080)\n    hconf.set(prefix + '.region', 'dallas')\n    hconf.setBoolean(prefix + '.public', False)\n\n# you can choose any name\nname = 'keystone'\nset_hadoop_config_with_credentials_f0d6ce325e0f4bc08812229b8d429dbe(name)\n", 
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "execution_count": 34, 
            "outputs": []
        }, 
        {
            "source": "# Leitura do arquivo no Hadoop\nfileNameOut = 'swift://'+ credentials_1['container'] + '.keystone/carros.csv' \ncarrosRDD = sc.textFile(fileNameOut)", 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "execution_count": 36, 
            "outputs": []
        }, 
        {
            "source": "# Colocando o RDD em cache. Esse processo otimiza a performance.\ncarrosRDD.cache()", 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "execution_count": 37, 
            "outputs": [
                {
                    "data": {
                        "text/plain": "swift://CursoSpark.keystone/carros.csv MapPartitionsRDD[49] at textFile at NativeMethodAccessorImpl.java:-2"
                    }, 
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "execution_count": 37
                }
            ]
        }, 
        {
            "source": "", 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "execution_count": 17, 
            "outputs": []
        }, 
        {
            "source": "carrosRDD.count()", 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "execution_count": 38, 
            "outputs": [
                {
                    "data": {
                        "text/plain": "399"
                    }, 
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "execution_count": 38
                }
            ]
        }, 
        {
            "source": "carrosRDD.take(5)", 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "execution_count": 39, 
            "outputs": [
                {
                    "data": {
                        "text/plain": "[u'MPG,CYLINDERS,DISPLACEMENT,HORSEPOWER,WEIGHT,ACCELERATION,MODELYEAR,NAME',\n u'18,8,307,130,3504,12,70,chevrolet chevelle malibu',\n u'15,8,350,165,3693,11.5,70,buick skylark 320',\n u'18,8,318,150,3436,11,70,plymouth satellite',\n u'16,8,304,150,3433,12,70,amc rebel sst']"
                    }, 
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "execution_count": 39
                }
            ]
        }, 
        {
            "source": "# Removendo a primeira linha do arquivo (cabe\u00e7alho)\ncarrosRDD2 = carrosRDD.filter(lambda x: \"DISPLACEMENT\" not in x)\ncarrosRDD2.count()", 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "execution_count": 40, 
            "outputs": [
                {
                    "data": {
                        "text/plain": "398"
                    }, 
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "execution_count": 40
                }
            ]
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "## Limpeza dos Dados", 
            "cell_type": "markdown"
        }, 
        {
            "source": "# Usando um valor padr\u00e3o para average HP (que ser\u00e1 usado para preencher os valores missing)\nmediaHP = sc.broadcast(75.0)", 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "execution_count": 41, 
            "outputs": []
        }, 
        {
            "source": "# Fun\u00e7\u00e3o para limpeza dos dados\ndef limpaDados( inputStr) :\n    global mediaHP\n    attList = inputStr.split(\",\")\n    \n    # Substitui o caracter ? por um valor\n    hpValue = attList[3]\n    if hpValue == \"?\":\n        hpValue = mediaHP.value\n       \n    # Cria uma linha usando a fun\u00e7\u00e3o Row, limpando e convertendo os dados de string para float\n    linhas = Row(MPG = float(attList[0]), CYLINDERS = float(attList[1]), DISPLACEMENT = float(attList[2]), \n                 HORSEPOWER = float(hpValue), WEIGHT = float(attList[4]), ACCELERATION = float(attList[5]), \n                 MODELYEAR = float(attList[6]), NAME = attList[7]) \n    return linhas", 
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "execution_count": 42, 
            "outputs": []
        }, 
        {
            "source": "# Executa a fun\u00e7\u00e3o no RDD\ncarrosRDD3 = carrosRDD2.map(limpaDados)\ncarrosRDD3.cache()\ncarrosRDD3.take(5)", 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "execution_count": 44, 
            "outputs": [
                {
                    "data": {
                        "text/plain": "[Row(ACCELERATION=12.0, CYLINDERS=8.0, DISPLACEMENT=307.0, HORSEPOWER=130.0, MODELYEAR=70.0, MPG=18.0, NAME=u'chevrolet chevelle malibu', WEIGHT=3504.0),\n Row(ACCELERATION=11.5, CYLINDERS=8.0, DISPLACEMENT=350.0, HORSEPOWER=165.0, MODELYEAR=70.0, MPG=15.0, NAME=u'buick skylark 320', WEIGHT=3693.0),\n Row(ACCELERATION=11.0, CYLINDERS=8.0, DISPLACEMENT=318.0, HORSEPOWER=150.0, MODELYEAR=70.0, MPG=18.0, NAME=u'plymouth satellite', WEIGHT=3436.0),\n Row(ACCELERATION=12.0, CYLINDERS=8.0, DISPLACEMENT=304.0, HORSEPOWER=150.0, MODELYEAR=70.0, MPG=16.0, NAME=u'amc rebel sst', WEIGHT=3433.0),\n Row(ACCELERATION=10.5, CYLINDERS=8.0, DISPLACEMENT=302.0, HORSEPOWER=140.0, MODELYEAR=70.0, MPG=17.0, NAME=u'ford torino', WEIGHT=3449.0)]"
                    }, 
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "execution_count": 44
                }
            ]
        }, 
        {
            "metadata": {}, 
            "source": "## An\u00e1lise Explorat\u00f3ria de Dados", 
            "cell_type": "markdown"
        }, 
        {
            "source": "# Cria um Dataframe\ncarrosDF = spSession.createDataFrame(carrosRDD3)", 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "execution_count": 21, 
            "outputs": [
                {
                    "ename": "Py4JJavaError", 
                    "output_type": "error", 
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", 
                        "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)", 
                        "\u001b[0;32m<ipython-input-21-10ed98d3265a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;31m# Cria um Dataframe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcarrosDF\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcarrosRDD3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m", 
                        "\u001b[0;32mc:\\Spark\\python\\pyspark\\sql\\session.pyc\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio)\u001b[0m\n\u001b[1;32m    522\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0mjrdd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSerDeUtil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoJavaArray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_to_java_object_rdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 524\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapplySchemaToPythonRDD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    525\u001b[0m         \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m         \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_schema\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32mc:\\Spark\\python\\lib\\py4j-0.10.1-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m         return_value = get_return_value(\n\u001b[0;32m--> 933\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m    934\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32mc:\\Spark\\python\\pyspark\\sql\\utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32mc:\\Spark\\python\\lib\\py4j-0.10.1-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    310\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    311\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    313\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m                 raise Py4JError(\n", 
                        "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o23.applySchemaToPythonRDD.\n: java.lang.RuntimeException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient\r\n\tat org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:522)\r\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:171)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat java.lang.reflect.Constructor.newInstance(Unknown Source)\r\n\tat org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:258)\r\n\tat org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:359)\r\n\tat org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:263)\r\n\tat org.apache.spark.sql.hive.HiveSharedState.metadataHive$lzycompute(HiveSharedState.scala:39)\r\n\tat org.apache.spark.sql.hive.HiveSharedState.metadataHive(HiveSharedState.scala:38)\r\n\tat org.apache.spark.sql.hive.HiveSharedState.externalCatalog$lzycompute(HiveSharedState.scala:46)\r\n\tat org.apache.spark.sql.hive.HiveSharedState.externalCatalog(HiveSharedState.scala:45)\r\n\tat org.apache.spark.sql.hive.HiveSessionState.catalog$lzycompute(HiveSessionState.scala:50)\r\n\tat org.apache.spark.sql.hive.HiveSessionState.catalog(HiveSessionState.scala:48)\r\n\tat org.apache.spark.sql.hive.HiveSessionState$$anon$1.<init>(HiveSessionState.scala:63)\r\n\tat org.apache.spark.sql.hive.HiveSessionState.analyzer$lzycompute(HiveSessionState.scala:63)\r\n\tat org.apache.spark.sql.hive.HiveSessionState.analyzer(HiveSessionState.scala:62)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:49)\r\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:64)\r\n\tat org.apache.spark.sql.SparkSession.applySchemaToPythonRDD(SparkSession.scala:666)\r\n\tat org.apache.spark.sql.SparkSession.applySchemaToPythonRDD(SparkSession.scala:656)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:280)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:128)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:211)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient\r\n\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1523)\r\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)\r\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)\r\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)\r\n\tat org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)\r\n\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)\r\n\tat org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)\r\n\t... 32 more\r\nCaused by: java.lang.reflect.InvocationTargetException\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat java.lang.reflect.Constructor.newInstance(Unknown Source)\r\n\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)\r\n\t... 38 more\r\nCaused by: java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: file:C:/Users/Rui/OneDrive/Backup/DataScience/CursoSpark/Capitulo9/Capitulo09/spark-warehouse\r\n\tat org.apache.hadoop.fs.Path.initialize(Path.java:205)\r\n\tat org.apache.hadoop.fs.Path.<init>(Path.java:171)\r\n\tat org.apache.hadoop.hive.metastore.Warehouse.getWhRoot(Warehouse.java:159)\r\n\tat org.apache.hadoop.hive.metastore.Warehouse.getDefaultDatabasePath(Warehouse.java:177)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB_core(HiveMetaStore.java:600)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:620)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)\r\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)\r\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)\r\n\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)\r\n\t... 43 more\r\nCaused by: java.net.URISyntaxException: Relative path in absolute URI: file:C:/Users/Rui/OneDrive/Backup/DataScience/CursoSpark/Capitulo9/Capitulo09/spark-warehouse\r\n\tat java.net.URI.checkPath(Unknown Source)\r\n\tat java.net.URI.<init>(Unknown Source)\r\n\tat org.apache.hadoop.fs.Path.initialize(Path.java:202)\r\n\t... 54 more\r\n"
                    ], 
                    "evalue": "An error occurred while calling o23.applySchemaToPythonRDD.\n: java.lang.RuntimeException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient\r\n\tat org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:522)\r\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:171)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat java.lang.reflect.Constructor.newInstance(Unknown Source)\r\n\tat org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:258)\r\n\tat org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:359)\r\n\tat org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:263)\r\n\tat org.apache.spark.sql.hive.HiveSharedState.metadataHive$lzycompute(HiveSharedState.scala:39)\r\n\tat org.apache.spark.sql.hive.HiveSharedState.metadataHive(HiveSharedState.scala:38)\r\n\tat org.apache.spark.sql.hive.HiveSharedState.externalCatalog$lzycompute(HiveSharedState.scala:46)\r\n\tat org.apache.spark.sql.hive.HiveSharedState.externalCatalog(HiveSharedState.scala:45)\r\n\tat org.apache.spark.sql.hive.HiveSessionState.catalog$lzycompute(HiveSessionState.scala:50)\r\n\tat org.apache.spark.sql.hive.HiveSessionState.catalog(HiveSessionState.scala:48)\r\n\tat org.apache.spark.sql.hive.HiveSessionState$$anon$1.<init>(HiveSessionState.scala:63)\r\n\tat org.apache.spark.sql.hive.HiveSessionState.analyzer$lzycompute(HiveSessionState.scala:63)\r\n\tat org.apache.spark.sql.hive.HiveSessionState.analyzer(HiveSessionState.scala:62)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:49)\r\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:64)\r\n\tat org.apache.spark.sql.SparkSession.applySchemaToPythonRDD(SparkSession.scala:666)\r\n\tat org.apache.spark.sql.SparkSession.applySchemaToPythonRDD(SparkSession.scala:656)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:280)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:128)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:211)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient\r\n\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1523)\r\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)\r\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)\r\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)\r\n\tat org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)\r\n\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)\r\n\tat org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)\r\n\t... 32 more\r\nCaused by: java.lang.reflect.InvocationTargetException\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat java.lang.reflect.Constructor.newInstance(Unknown Source)\r\n\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)\r\n\t... 38 more\r\nCaused by: java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: file:C:/Users/Rui/OneDrive/Backup/DataScience/CursoSpark/Capitulo9/Capitulo09/spark-warehouse\r\n\tat org.apache.hadoop.fs.Path.initialize(Path.java:205)\r\n\tat org.apache.hadoop.fs.Path.<init>(Path.java:171)\r\n\tat org.apache.hadoop.hive.metastore.Warehouse.getWhRoot(Warehouse.java:159)\r\n\tat org.apache.hadoop.hive.metastore.Warehouse.getDefaultDatabasePath(Warehouse.java:177)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB_core(HiveMetaStore.java:600)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:620)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)\r\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)\r\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)\r\n\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)\r\n\t... 43 more\r\nCaused by: java.net.URISyntaxException: Relative path in absolute URI: file:C:/Users/Rui/OneDrive/Backup/DataScience/CursoSpark/Capitulo9/Capitulo09/spark-warehouse\r\n\tat java.net.URI.checkPath(Unknown Source)\r\n\tat java.net.URI.<init>(Unknown Source)\r\n\tat org.apache.hadoop.fs.Path.initialize(Path.java:202)\r\n\t... 54 more\r\n"
                }
            ]
        }, 
        {
            "source": "# Estat\u00edsticas descritivas\ncarrosDF.select(\"MPG\",\"CYLINDERS\").describe().show()", 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "execution_count": 12, 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+-------+-----------------+------------------+\n|summary|              MPG|         CYLINDERS|\n+-------+-----------------+------------------+\n|  count|              398|               398|\n|   mean|23.51457286432161| 5.454773869346734|\n| stddev|7.815984312565782|1.7010042445332125|\n|    min|              9.0|               3.0|\n|    max|             46.6|               8.0|\n+-------+-----------------+------------------+\n\n"
                }
            ]
        }, 
        {
            "source": "# Encontrando a correla\u00e7\u00e3o entre a vari\u00e1vel target com as vari\u00e1veis preditoras\nfor i in carrosDF.columns:\n    if not(isinstance(carrosDF.select(i).take(1)[0][0], str)) :\n        print( \"Correla\u00e7\u00e3o da vari\u00e1vel MPG com \", i, carrosDF.stat.corr('MPG', i))", 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "execution_count": 13, 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "Correla\u00e7\u00e3o da vari\u00e1vel MPG com  ACCELERATION 0.4202889121016499\nCorrela\u00e7\u00e3o da vari\u00e1vel MPG com  CYLINDERS -0.7753962854205548\nCorrela\u00e7\u00e3o da vari\u00e1vel MPG com  DISPLACEMENT -0.8042028248058979\nCorrela\u00e7\u00e3o da vari\u00e1vel MPG com  HORSEPOWER -0.7747041523498721\nCorrela\u00e7\u00e3o da vari\u00e1vel MPG com  MODELYEAR 0.5792671330833091\nCorrela\u00e7\u00e3o da vari\u00e1vel MPG com  MPG 1.0\nCorrela\u00e7\u00e3o da vari\u00e1vel MPG com  WEIGHT -0.8317409332443347\n"
                }
            ]
        }, 
        {
            "metadata": {}, 
            "source": "## Pr\u00e9-Processamento dos Dados", 
            "cell_type": "markdown"
        }, 
        {
            "source": "# Convertendo para um LabeledPoint (target, Vector[features])\n# Remove colunas n\u00e3o relevantes para o modelo ou com baixa correla\u00e7\u00e3o\ndef transformaVar(row) :\n    obj = (row[\"MPG\"], Vectors.dense([row[\"ACCELERATION\"], row[\"DISPLACEMENT\"], row[\"WEIGHT\"]]))\n    return obj", 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "execution_count": 14, 
            "outputs": []
        }, 
        {
            "source": "# Utiliza o RDD, aplica a fun\u00e7\u00e3o, converte para Dataframe e aplica a fun\u00e7\u00e3o select()\ncarrosRDD4 = carrosRDD3.map(transformaVar)\ncarrosDF = spSession.createDataFrame(carrosRDD4,[\"label\", \"features\"])\ncarrosDF.select(\"label\",\"features\").show(10)", 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "execution_count": 15, 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+-----+-------------------+\n|label|           features|\n+-----+-------------------+\n| 18.0|[12.0,307.0,3504.0]|\n| 15.0|[11.5,350.0,3693.0]|\n| 18.0|[11.0,318.0,3436.0]|\n| 16.0|[12.0,304.0,3433.0]|\n| 17.0|[10.5,302.0,3449.0]|\n| 15.0|[10.0,429.0,4341.0]|\n| 14.0| [9.0,454.0,4354.0]|\n| 14.0| [8.5,440.0,4312.0]|\n| 14.0|[10.0,455.0,4425.0]|\n| 15.0| [8.5,390.0,3850.0]|\n+-----+-------------------+\nonly showing top 10 rows\n\n"
                }
            ]
        }, 
        {
            "source": "carrosRDD4.take(5)", 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "execution_count": 16, 
            "outputs": [
                {
                    "data": {
                        "text/plain": "[(18.0, DenseVector([12.0, 307.0, 3504.0])),\n (15.0, DenseVector([11.5, 350.0, 3693.0])),\n (18.0, DenseVector([11.0, 318.0, 3436.0])),\n (16.0, DenseVector([12.0, 304.0, 3433.0])),\n (17.0, DenseVector([10.5, 302.0, 3449.0]))]"
                    }, 
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "execution_count": 16
                }
            ]
        }, 
        {
            "metadata": {}, 
            "source": "## Machine Learning", 
            "cell_type": "markdown"
        }, 
        {
            "source": "# Dados de Treino e de Teste\n(dados_treino, dados_teste) = carrosDF.randomSplit([0.7, 0.3])", 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "execution_count": 17, 
            "outputs": []
        }, 
        {
            "source": "dados_treino.count()", 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "execution_count": 18, 
            "outputs": [
                {
                    "data": {
                        "text/plain": "287"
                    }, 
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "execution_count": 18
                }
            ]
        }, 
        {
            "source": "dados_teste.count()", 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "execution_count": 19, 
            "outputs": [
                {
                    "data": {
                        "text/plain": "111"
                    }, 
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "execution_count": 19
                }
            ]
        }, 
        {
            "source": "# Construindo o modelo com os dados de treino\nlinearReg = LinearRegression(maxIter = 10)\nmodelo = linearReg.fit(dados_treino)", 
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "execution_count": 20, 
            "outputs": []
        }, 
        {
            "source": "print(modelo)", 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "execution_count": 21, 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "LinearRegression_4b718f39d15e4655d3b4\n"
                }
            ]
        }, 
        {
            "source": "# Imprimindo as m\u00e9tricas\nprint(\"Coeficientes: \" + str(modelo.coefficients))\nprint(\"Intercept: \" + str(modelo.intercept))", 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "execution_count": 22, 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "Coeficientes: [0.20683227091,-0.0139156296053,-0.00571177718642]\nIntercept: 40.070468428318414\n"
                }
            ]
        }, 
        {
            "source": "# Previs\u00f5es com dados de teste\npredictions = modelo.transform(dados_teste)\npredictions.select(\"prediction\", \"features\").show()", 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "execution_count": 23, 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+------------------+-------------------+\n|        prediction|           features|\n+------------------+-------------------+\n| 9.913154602969094|[11.0,429.0,4633.0]|\n| 7.735003087949785|[11.0,455.0,4951.0]|\n|11.488830476938418|[12.0,400.0,4464.0]|\n|11.832141254222886|[12.5,400.0,4422.0]|\n|14.650893383592912|[16.0,302.0,4294.0]|\n|14.290973955959721|[13.0,351.0,4129.0]|\n| 15.78001282820481|[14.5,302.0,4042.0]|\n|14.730981744824629|[15.5,304.0,4257.0]|\n|18.506122537199946|[11.0,318.0,3399.0]|\n| 16.48497603247754|[11.5,350.0,3693.0]|\n|16.657319167099608|[12.5,318.0,3777.0]|\n|14.573343113326626|[13.0,350.0,4082.0]|\n|14.819335205272605|[13.5,318.0,4135.0]|\n|12.735359151499622|[14.0,350.0,4440.0]|\n|21.053220938567524|[17.0,250.0,3336.0]|\n|21.332219412311503|[21.0,250.0,3432.0]|\n|12.034140356336867| [9.5,400.0,4278.0]|\n|21.591336286289643|[18.0,250.0,3278.0]|\n|18.159162343353028|[18.5,250.0,3897.0]|\n|20.782954972507465|[15.5,250.0,3329.0]|\n+------------------+-------------------+\nonly showing top 20 rows\n\n"
                }
            ]
        }, 
        {
            "source": "# Coeficiente de determina\u00e7\u00e3o R2\navaliador = RegressionEvaluator(predictionCol = \"prediction\", labelCol = \"label\", metricName = \"r2\")\navaliador.evaluate(predictions)", 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "execution_count": 24, 
            "outputs": [
                {
                    "data": {
                        "text/plain": "0.7057954428371136"
                    }, 
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "execution_count": 24
                }
            ]
        }, 
        {
            "metadata": {}, 
            "source": "# Fim", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "### Obrigado - Data Science Academy - <a href=http://facebook.com/dsacademy>facebook.com/dsacademybr</a>", 
            "cell_type": "markdown"
        }
    ]
}